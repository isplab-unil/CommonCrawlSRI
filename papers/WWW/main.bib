
@inproceedings{soni_sicilian_2015,
  address = {{Denver, Colorado, USA}},
  title = {The {{SICILIAN Defense}}: {{Signature}}-Based {{Whitelisting}} of {{Web JavaScript}}},
  isbn = {978-1-4503-3832-5},
  shorttitle = {The {{SICILIAN Defense}}},
  language = {en},
  booktitle = {Proc. of the {{ACM Conf}}. on {{Computer}} and {{Communications Security}} ({{CCS}})},
  publisher = {{ACM}},
  doi = {10.1145/2810103.2813710},
  author = {Soni, Pratik and Budianto, Enrico and Saxena, Prateek},
  year = {2015},
  pages = {1542-1557},
  file = {/Users/chapuisbertil/Zotero/storage/E9SSB9J4/Soni et al. - 2015 - The SICILIAN Defense Signature-based Whitelisting.pdf}
}

@article{shah_securing_2017,
  title = {Securing {{Third}}-Party {{Web Resources Using Subresource Integrity Automation}}},
  abstract = {Using Content Delivery Networks (CDNs) to host files such as scripts and stylesheets that are shared among multiple sites can improve site performance and conserve bandwidth. However, using CDNs also comes with a risk, in that if an attacker gains control of a CDN, the attacker can inject arbitrary malicious content into files on the CDN and thus can also potentially attack all sites that fetch files from that CDN. Internet security and its awareness is an often discussed topic these days. The diversity and the potential of current web browser applications has highly increased in the last years. With this, the way of how security of such web pages is rated to the users has changed as well. In order to avoid cross scripting attacks we need to authenticate resources that we are fetching from CDN. This work especially address these cross scripting attacks and measures to avoid it. The Subresource Integrity feature that is announced as W3C recommendation on the 23rd of June 2016 is not still implemented by major portion of user. This work makes it easier for even novice user to use SRI mechanism to protect himself/herself from different kinds of security breaches.},
  language = {en},
  journal = {International Journal on Emerging Trends in Technology},
  author = {Shah, Ronak N and Patil, Kailas R},
  year = {2017},
  pages = {5},
  file = {/Users/chapuisbertil/Zotero/storage/AMH65B4N/Shah and Patil - 2017 - Securing Third-party Web Resources Using Subresour.pdf}
}

@article{shah_measurement_2018,
  title = {A {{Measurement Study}} of the {{Subresource Integrity Mechanism}} on {{Real}}-World {{Applications}}},
  volume = {13},
  issn = {1747-8405, 1747-8413},
  language = {en},
  number = {2},
  journal = {International Journal of Security and Networks},
  doi = {10.1504/IJSN.2018.092474},
  author = {Shah, Ronak and Patil, Kailas},
  year = {2018},
  pages = {129},
  file = {/Users/chapuisbertil/Zotero/storage/CYS3HZVS/Shah and Patil - 2018 - A measurement study of the subresource integrity m.pdf}
}

@inproceedings{cherubini_towards_2018,
  address = {{Toronto, ON, Canada}},
  title = {Towards {{Usable Checksums}}: {{Automating}} the {{Integrity Verification}} of {{Web Downloads}} for the {{Masses}}},
  booktitle = {Proc. of the {{ACM Conf}}. on {{Computer}} and {{Communications Security}} ({{CCS}})},
  publisher = {{ACM}},
  doi = {10.1145/3243734.3243746},
  author = {Cherubini, Mauro and Meylan, Alexandre and Chapuis, Bertil and Humbert, Mathias and Bilogrevic, Igor and Huguenin, K{\'e}vin},
  month = oct,
  year = {2018},
  pages = {1256-1271},
  file = {/Users/chapuisbertil/Zotero/storage/Q5RTE7H7/Cherubini et al. - 2018 - Towards Usable Checksums Automating the Integrity.pdf}
}

@misc{w3c_subresource_2016,
  title = {Subresource {{Integrity}}},
  language = {en},
  howpublished = {https://www.w3.org/TR/SRI/},
  author = {{W3C}},
  year = {2016},
  file = {/Users/chapuisbertil/Zotero/storage/RECSMVR4/SRI.html}
}

@inproceedings{felt_improving_2015,
  title = {Improving {{SSL Warnings}}: {{Comprehension}} and {{Adherence}}},
  isbn = {978-1-4503-3145-6},
  shorttitle = {Improving {{SSL Warnings}}},
  abstract = {Browsers warn users when the privacy of an SSL/TLS connection might be at risk. An ideal SSL warning would empower users to make informed decisions and, failing that, guide confused users to safety. Unfortunately, users struggle to understand and often disregard real SSL warnings. We report on the task of designing a new SSL warning, with the goal of improving comprehension and adherence. We designed a new SSL warning based on recommendations from warning literature and tested our proposal with microsurveys and a field experiment. We ultimately failed at our goal of a well-understood warning. However, nearly 30\% more total users chose to remain safe after seeing our warning. We attribute this success to opinionated design, which promotes safety with visual cues. Subsequently, our proposal was released as the new Google Chrome SSL warning. We raise questions about warning comprehension advice and recommend that other warning designers use opinionated design.},
  booktitle = {Proc. of the {{ACM Conf}}. on {{Human Factors}} in {{Computing Systems}} ({{CHI}})},
  publisher = {{ACM}},
  doi = {10.1145/2702123.2702442},
  author = {Felt, Adrienne Porter and Ainslie, Alex and Reeder, Robert W. and Consolvo, Sunny and Thyagaraja, Somas and Bettes, Alan and Harris, Helen and Grimes, Jeff},
  year = {2015},
  keywords = {security,warnings,design,google consumer surveys,https,microsurveys,ssl,tls/ssl},
  pages = {2893--2902},
  file = {/Users/chapuisbertil/Zotero/storage/EWPBRPFG/Felt et al. - 2015 - Improving SSL Warnings Comprehension and Adherenc.pdf}
}

@inproceedings{ruohonen_integrity_2018,
  series = {{{IFIP Advances}} in {{Information}} and {{Communication Technology}}},
  title = {On the {{Integrity}} of {{Cross}}-{{Origin JavaScripts}}},
  isbn = {978-3-319-99828-2},
  abstract = {The same-origin policy is a fundamental part of the Web. Despite the restrictions imposed by the policy, embedding of third-party JavaScript code is allowed and commonly used. Nothing is guaranteed about the integrity of such code. To tackle this deficiency, solutions such as the subresource integrity standard have been recently introduced. Given this background, this paper presents the first empirical study on the temporal integrity of cross-origin JavaScript code. According to the empirical results based on a ten day polling period of over 35 thousand scripts collected from popular websites, (i) temporal integrity changes are relatively common; (ii) the adoption of the subresource integrity standard is still in its infancy; and (iii) it is possible to statistically predict whether a temporal integrity change is likely to occur. With these results and the accompanying discussion, the paper contributes to the ongoing attempts to better understand security and privacy in the current Web.},
  language = {en},
  booktitle = {{{ICT Systems Security}} and {{Privacy Protection}}},
  publisher = {{Springer}},
  author = {Ruohonen, Jukka and Salovaara, Joonas and Lepp{\"a}nen, Ville},
  editor = {Janczewski, Lech Jan and Kuty{\l}owski, Miros{\l}aw},
  year = {2018},
  keywords = {Cross-domain,Remote inclusion,Same-origin,Subresource integrity},
  pages = {385-398},
  file = {/Users/chapuisbertil/Zotero/storage/KXPVLNPR/Ruohonen et al. - 2018 - On the Integrity of Cross-Origin JavaScripts.pdf}
}

@misc{king_observatory_nodate,
  title = {Observatory by {{Mozilla}}},
  abstract = {Observatory by Mozilla is a project designed to help developers, system administrators, and security professionals configure their sites safely and securely.},
  language = {en},
  howpublished = {https://observatory.mozilla.org/},
  author = {King, April},
  file = {/Users/chapuisbertil/Zotero/storage/DX94BHU8/observatory.mozilla.org.html}
}

@inproceedings{van_acker_measuring_2017,
  title = {Measuring {{Login Webpage Security}}},
  isbn = {978-1-4503-4486-9},
  abstract = {Login webpages are the entry points into sensitive parts of web applications, dividing between public access to a website and private, user-specific, access to the website resources. As such, these entry points must be guarded with great care. A vast majority of today's websites relies on text-based user-name/password pairs for user authentication. While much prior research has focused on the strengths and weaknesses of textual passwords, this paper puts a spotlight on the security of the login webpages themselves. We conduct an empirical study of the Alexa top 100,000 pages to identify login pages and scrutinize their security. Our findings show several widely spread vulnerabilities, such as possibilities for password leaks to third parties and password eavesdropping on the network. They also show that only a scarce number of login pages deploy advanced security measures. Our findings on open-source web frameworks and content management systems confirm the lack of support against the login attacker. To ameliorate the problematic state of the art, we discuss measures to improve the security of login pages.},
  booktitle = {Proc. of the {{ACM Symp}}. on {{Applied Computing}} ({{SAC}})},
  publisher = {{ACM}},
  doi = {10.1145/3019612.3019798},
  author = {Van Acker, Steven and Hausknecht, Daniel and Sabelfeld, Andrei},
  year = {2017},
  keywords = {web security,attacker models,large-scale study,login page},
  pages = {1753--1760},
  file = {/Users/chapuisbertil/Zotero/storage/69NB7JL7/Van Acker et al. - 2017 - Measuring Login Webpage Security.pdf}
}

@misc{eden_major_2018,
  title = {Major {{Sites Running Unauthenticated JavaScript}} on Their {{Payment Pages}}},
  abstract = {A few months ago, British Airways' customers had their credit card details stolen. How was this possible? The best guess goes something like this: BA had 3rd party JS on its payment page The \ldots{}},
  language = {en-GB},
  journal = {Terence Eden's Blog},
  howpublished = {https://shkspr.mobi/blog/2018/11/major-sites-running-unauthenticated-javascript-on-their-payment-pages/},
  author = {Eden, Terence},
  year = {2018},
  file = {/Users/chapuisbertil/Zotero/storage/F282ZQSV/2018 - Major sites running unauthenticated JavaScript on .html;/Users/chapuisbertil/Zotero/storage/QAXVGIBM/item.html}
}

@incollection{matulevicius_ensuring_2018,
  title = {Ensuring {{Resource Trust}} and {{Integrity}} in {{Web Browsers Using Blockchain Technology}}},
  volume = {316},
  isbn = {978-3-319-92897-5 978-3-319-92898-2},
  abstract = {Current web technology allows the use of cryptographic primitives as part of server-provided Javascript. This may result in security problems with web-based services. We provide an example for an attack on the WhisperKey service. We present a solution which is based on human code reviewing and on CVE (Common Vulnerabilities and Exposures) data bases. In our approach, existing code audits and known vulnerabilities are tied to the Javascript file by a tamper-proof Blockchain approach and are signaled to the user by a browser extension. The contribution explains our concept and its workflow; it may be extended to all situations with modular, mobile code. Finally, we propose an amendment to the W3C subresource recommendation.},
  language = {en},
  booktitle = {Advanced {{Information Systems Engineering Workshops}}},
  publisher = {{Springer}},
  author = {Cap, Clemens H. and Leiding, Benjamin},
  editor = {Matulevi{\v c}ius, Raimundas and Dijkman, Remco},
  year = {2018},
  pages = {115-125},
  file = {/Users/chapuisbertil/Zotero/storage/PIJFYJ79/Cap and Leiding - 2018 - Ensuring Resource Trust and Integrity in Web Brows.pdf},
  doi = {10.1007/978-3-319-92898-2_9}
}

@inproceedings{kerschbaumer_enforcing_2016,
  address = {{Boston, MA, USA}},
  title = {Enforcing {{Content Security}} by {{Default}} within {{Web Browsers}}},
  isbn = {978-1-5090-5589-0},
  abstract = {Web browsers were initially designed to retrieve resources on the world wide web in a static manner such that adding security checks in select locations throughout the codebase sufficiently provided the necessary security guarantees of the web. Even though systematic security checks were always performed, those security checks were sprinkled throughout the codebase. Over time, various specifications for dynamically loading content have proven that such a scattered security model is error-prone. Instead of opting into security checks wherever resource loads are initiated throughout the codebase, we present an approach where security checks are performed by default. By equipping every resource load with a loading context (which includes information about who initiated the load, the load type, etc.), our approach enforces an opt-out security mechanism performing security checks by default by consulting a centralized security manager. In addition, the added load context allows to provide the same security guarantees for resource loads which encounter a server-side redirect.},
  language = {en},
  booktitle = {Proc. of the {{IEEE Conf}}. on {{Cybersecurity Development}} ({{SecDev}})},
  publisher = {{IEEE}},
  doi = {10.1109/SecDev.2016.033},
  author = {Kerschbaumer, Christoph},
  year = {2016},
  pages = {101-106},
  file = {/Users/chapuisbertil/Zotero/storage/L828N6X2/Kerschbaumer - 2016 - Enforcing Content Security by Default within Web B.pdf}
}

@inproceedings{afanasyev_content-based_2016,
  title = {Content-Based Security for the Web},
  isbn = {978-1-4503-4813-3},
  abstract = {The World Wide Web has become the most common platform for building applications and delivering content. Yet despite years of research, the web continues to face severe security challenges related to data integrity and confidentiality. Rather than continuing the exploit-and-patch cycle, we propose addressing these challenges at an architectural level, by supplementing the web's existing connection-based and server-based security models with a new approach: contentbased security. With this approach, content is directly signed and encrypted at rest, enabling it to be delivered via any path and then validated by the browser. We explore how this new architectural approach can be applied to the web and analyze its security benefits. We then discuss a broad research agenda to realize this vision and the challenges that must be overcome.},
  booktitle = {Proc. of the {{New Security Paradigms Workshop}}},
  publisher = {{ACM}},
  doi = {10.1145/3011883.3011890},
  author = {Afanasyev, Alexander and Halderman, J. Alex and Ruoti, Scott and Seamons, Kent and Yu, Yingdi and Zappala, Daniel and Zhang, Lixia},
  year = {2016},
  pages = {49-60},
  file = {/Users/chapuisbertil/Zotero/storage/VS844SMW/Afanasyev et al. - 2016 - Content-based security for the web.pdf}
}

@inproceedings{salvador_wraudit_2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Wraudit: {{A Tool}} to {{Transparently Monitor Web Resources}}' {{Integrity}}},
  isbn = {978-3-030-05918-7},
  shorttitle = {Wraudit},
  abstract = {JavaScript has become the language of reference for programming the client-side logics of web-applications. However, there is no native full support to protect the integrity of this code from modifications conducted by the server where it is hosted. Many election applications, including internet voting solutions, are based on this language. Thus, if the server that hosts the code is compromised, a modified version of the code could be served and some election security properties affected, e.g. voter privacy, vote integrity, and so on. Furthermore, the usage of a Content Delivery Network (CDN) to mitigate Distributed Denial-of-Service (DDoS) attacks on internet voting solutions has been called into question for similar reasons. A malicious administrator of the hosting provider could have the opportunity to modify JavaScript files affecting the web application's code integrity. In order to tackle this problem, in this article we propose a solution that mitigates those risks by using a service called wraudit that transparently monitors the integrity of the published code. The service design and implementation are presented and the first insights from our experience using it are explained.},
  language = {en},
  booktitle = {Mining {{Intelligence}} and {{Knowledge Exploration}}},
  publisher = {{Springer}},
  author = {Salvador, David and Cucurull, Jordi and Juli{\`a}, Pau},
  editor = {Groza, Adrian and Prasath, Rajendra},
  year = {2018},
  keywords = {Code integrity,Experience,JavaScript,Monitoring tool,Web resources},
  pages = {239-247}
}

@inproceedings{fett_spresso_2015,
  title = {{{SPRESSO}}: {{A Secure}}, {{Privacy}}-{{Respecting Single Sign}}-{{On System}} for the {{Web}}},
  isbn = {978-1-4503-3832-5},
  abstract = {Single sign-on (SSO) systems, such as OpenID and OAuth, allow web sites, so-called relying parties (RPs), to delegate user authentication to identity providers (IdPs), such as Facebook or Google. These systems are very popular, as they provide a convenient means for users to log in at RPs and move much of the burden of user authentication from RPs to IdPs. There is, however, a downside to current systems, as they do not respect users' privacy: IdPs learn at which RP a user logs in. With one exception, namely Mozilla's BrowserID system (a.k.a. Mozilla Persona), current SSO systems were not even designed with user privacy in mind. Unfortunately, recently discovered attacks, which exploit design flaws of BrowserID, show that BrowserID does not provide user privacy either. In this paper, we therefore propose the first privacy-respecting SSO system for the web, called SPRESSO (for Secure Privacy-REspecting Single Sign-On). The system is easy to use, decentralized, and platform independent. It is based solely on standard HTML5 and web features and uses no browser extensions, plug-ins, or other executables. Existing SSO systems and the numerous attacks on such systems illustrate that the design of secure SSO systems is highly non-trivial. We therefore also carry out a formal analysis of SPRESSO based on an expressive model of the web in order to formally prove that SPRESSO enjoys strong authentication and privacy properties.},
  booktitle = {Proc. of the {{ACM Conf}}. on {{Computer}} and {{Communications Security}} ({{CCS}})},
  publisher = {{ACM}},
  doi = {10.1145/2810103.2813726},
  author = {Fett, Daniel and K{\"u}sters, Ralf and Schmitz, Guido},
  year = {2015},
  keywords = {formal analysis,HTML5,privacy,single sign-on,web security},
  pages = {1358--1369},
  file = {/Users/chapuisbertil/Zotero/storage/P475TEY5/Fett et al. - 2015 - SPRESSO A Secure, Privacy-Respecting Single Sign-.pdf}
}

@inproceedings{kumar_security_2017,
  title = {Security {{Challenges}} in an {{Increasingly Tangled Web}}},
  isbn = {978-1-4503-4913-0},
  abstract = {Over the past 20 years, websites have grown increasingly complex and interconnected. In 2016, only a negligible number of sites are dependency free, and over 90\% of sites rely on external content. In this paper, we investigate the current state of web dependencies and explore two security challenges associated with the increasing reliance on external services: (1) the expanded attack surface associated with serving unknown, implicitly trusted third-party content, and (2) how the increased set of external dependencies impacts HTTPS adoption. We hope that by shedding light on these issues, we can encourage developers to consider the security risks associated with serving third-party content and prompt service providers to more widely deploy HTTPS.},
  booktitle = {Proc. of the {{Int}}'l {{Conf}} on {{World Wide Web}} ({{WWW}})},
  publisher = {{ACM}},
  doi = {10.1145/3038912.3052686},
  author = {Kumar, Deepak and Ma, Zane and Durumeric, Zakir and Mirian, Ariana and Mason, Joshua and Halderman, J. Alex and Bailey, Michael},
  year = {2017},
  keywords = {https adoption,privacy/tracking,website complexity},
  pages = {677--684},
  file = {/Users/chapuisbertil/Zotero/storage/H9M7LE8Z/Kumar et al. - 2017 - Security Challenges in an Increasingly Tangled Web.pdf}
}

@inproceedings{anis_securing_2018,
  title = {Securing {{Web Applications}} with {{Secure Coding Practices}} and {{Integrity Verification}}},
  abstract = {The concept of security in web applications is not new. However, it is often ignored in the development stages of the applications. Being multitiered and spread across different domains, it is challenging to come up with a security solution that works for all web applications. Moreover, developers are more inclined to implement features and often do not practice secure coding. Therefore, countless web applications are launched with security vulnerabilities like cross-site scripting, injection attacks and resource alterations. In addition, code tampering on the client side is a serious security risk for web applications. In our opinion, integrating security features should be a part of the development process. Without practicing secure coding and having an integrity verification system in place, it is difficult to defend security attacks. In this paper, we present a system that helps developers to implement security measures on the client side code based on the best practices of secure coding. We also develop an integrity verification module to prevent code tampering attacks on the client side. The proposed approach can be integrated with both new and existing web applications. We implement our approach for a number of JavaScript-based applications and the results show that our approach increased the security of the applications and prevented any modifications performed on the client side.},
  booktitle = {Proc. of the {{IEEE Int}}'l {{Conf}}. on {{Dependable}}, {{Autonomic}} and {{Secure Computing}}, {{Int}}'l {{Conf}} on {{Pervasive Intelligence}} and {{Computing}}, 4th {{Int}}'l {{Conf}} on {{Big Data Intelligence}} and {{Computing}} and {{Cyber Science}} and {{Technology Congress}}({{DASC}}/{{PiCom}}/{{DataCom}}/{{CyberSciTech}})},
  publisher = {{IEEE}},
  doi = {10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00112},
  author = {Anis, A. and Zulkernine, M. and Iqbal, S. and Liem, C. and Chambers, C.},
  year = {2018},
  keywords = {Internet,security of data,Application security,code tampering attacks,countless web applications,cross-site scripting,Cross-site scripting,Encoding,formal verification,integrity verification module,integrity verification system,JavaScript-based applications,Runtime,secure coding practices,security attacks,security features,security measures,security risk,security solution,security vulnerabilities,Servers,SQL injection,Web Application Security; Secure Coding; Integrity Verification},
  pages = {618-625},
  file = {/Users/chapuisbertil/Zotero/storage/WJNVYFTT/Anis et al. - 2018 - Securing Web Applications with Secure Coding Pract.pdf}
}

@inproceedings{lavrenovs_http_2018,
  title = {{{HTTP Security Headers Analysis}} of {{Top One Million Websites}}},
  abstract = {We present research on the security of the most popular websites, ranked according to Alexa's top one million list, based on an HTTP response headers analysis. For each of the domains included in the list, we made four different requests: an HTTP/1.1 request to the domain itself and to its "www" subdomain and two more equivalent HTTPS requests. Redirections were always followed. A detailed discussion of the request process and main outcomes is presented, including X.509 certificate issues and comparison of results with equivalent HTTP/2 requests. The body of the responses was discarded, and the HTTP response header fields were stored in a database. We analysed the prevalence of the most important response headers related to web security aspects. In particular, we took into account Strict- Transport-Security, Content-Security-Policy, X-XSS-Protection, X-Frame-Options, Set-Cookie (for session cookies) and X-Content-Type. We also reviewed the contents of response HTTP headers that potentially could reveal unwanted information, like Server (and related headers), Date and Referrer-Policy. This research offers an up-to-date survey of current prevalence of web security policies implemented through HTTP response headers and concludes that most popular sites tend to implement it noticeably more often than less popular ones. Equally, HTTPS sites seem to be far more eager to implement those policies than HTTP only websites. A comparison with previous works show that web security policies based on HTTP response headers are continuously growing, but still far from satisfactory widespread adoption.},
  booktitle = {Int'l {{Conf}}. on {{Cyber Conflict}} ({{CyCon}})},
  publisher = {{IEEE}},
  doi = {10.23919/CYCON.2018.8405025},
  author = {Lavrenovs, A. and Mel{\'o}n, F. J. R.},
  year = {2018},
  keywords = {web security,Alexa top one million list,Charge coupled devices,Content Security Policy,content-security-policy,Databases,HTTP headers,HTTP response headers analysis,HTTP security headers analysis,HTTP Strict Transport Security,HTTP/2,HTTPS,HTTPS requests,hypermedia,Internet,Protocols,redirections,referrer-policy,Security,security of data,server,set-cookie,strict- transport-security,Tools,top one million websites survey,transport protocols,Uniform resource locators,web security policies,Web sites,websites,World Wide Web,www subdomain,X-content-type,X-frame-options,X-XSS-protection,X.509 certificate},
  pages = {345-370},
  file = {/Users/chapuisbertil/Zotero/storage/MLKG4NKZ/Lavrenovs and Melón - 2018 - HTTP Security Headers Analysis of Top One Million .pdf}
}

@misc{cisco_cisco_nodate,
  title = {Cisco {{Popularity List}}},
  howpublished = {http://s3-us-west-1.amazonaws.com/umbrella-static/index.html},
  author = {Cisco},
  file = {/Users/chapuisbertil/Zotero/storage/I77JM6RD/index.html}
}

@inproceedings{jansen_brief_2017,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Brief {{Announcement}}: {{Federated Code Auditing}} and {{Delivery}} for {{MPC}}},
  isbn = {978-3-319-69084-1},
  shorttitle = {Brief {{Announcement}}},
  abstract = {Secure multi-party computation (MPC) is a cryptographic primitive that enables several parties to compute jointly over their collective private data sets. MPC's objective is to federate trust over several computing entities such that a large threshold (e.g., a majority) must collude before sensitive or private input data can be breached. Over the past decade, several general and special-purpose software frameworks have been developed that provide data contributors with control over deciding whom to trust to perform the calculation and (separately) to receive the output. However, one crucial component remains centralized within all existing MPC frameworks: the distribution of the MPC software application itself. For desktop applications, trust in the code must be determined once at download time. For web-based JavaScript applications subject to trust on every use, all data contributors across several invocations of MPC must maintain centralized trust in a single code delivery service. In this work, we design and implement a federated code delivery mechanism for web-based MPC such that data contributors only execute code that has been accredited by several trusted auditors (the contributor aborts if consensus is not reached). Our client-side Chrome browser extension is independent of any MPC scheme and has a trusted computing base of fewer than 100 lines of code.},
  language = {en},
  booktitle = {Stabilization, {{Safety}}, and {{Security}} of {{Distributed Systems}}},
  publisher = {{Springer}},
  author = {Jansen, Frederick and Albab, Kinan Dak and Lapets, Andrei and Varia, Mayank},
  editor = {Spirakis, Paul and Tsigas, Philippas},
  year = {2017},
  keywords = {Content delivery,Secure multi-party computation,Web security},
  pages = {298-302},
  file = {/Users/chapuisbertil/Zotero/storage/DXFWKKIP/Jansen et al. - 2017 - Brief Announcement Federated Code Auditing and De.pdf}
}

@inproceedings{lauinger_thou_2017,
  title = {Thou {{Shalt Not Depend}} on {{Me}}: {{Analysing}} the {{Use}} of {{Outdated JavaScript Libraries}} on the {{Web}}},
  isbn = {978-1-891562-46-4},
  abstract = {Web developers routinely rely on third-party JavaScript libraries such as jQuery to enhance the functionality of their sites. However, if not properly maintained, such dependencies can create attack vectors allowing a site to be compromised.},
  language = {en},
  booktitle = {Proc. of the {{Symp}}. on {{Network}} and {{Distributed System Security}} ({{NDSS}})},
  doi = {10.14722/ndss.2017.23414},
  author = {Lauinger, Tobias and Chaabane, Abdelberi and Arshad, Sajjad and Robertson, William and Wilson, Christo and Kirda, Engin},
  year = {2017},
  file = {/Users/chapuisbertil/Zotero/storage/5U62WA74/Lauinger et al. - 2017 - Thou Shalt Not Depend on Me Analysing the Use of .pdf}
}

@misc{amazon_alexa_nodate,
  title = {Alexa {{Top}} 1 {{Million}}},
  abstract = {It's time! August 2018 represents the 7th time I've published a report of the Alexa Top 1 Million sites so let's get stuck in and see what changes have taken place over the last six months on the biggest sites on the web. Crawler data As always the data from},
  language = {en},
  journal = {Scott Helme},
  howpublished = {https://scotthelme.co.uk/alexa-top-1-million-analysis-august-2018/},
  author = {Amazon},
  file = {/Users/chapuisbertil/Zotero/storage/Z2E8ZB7J/alexa-top-1-million-analysis-august-2018.html}
}

@inproceedings{bashir_tracing_2016,
  title = {Tracing {{Information Flows Between Ad Exchanges Using Retargeted Ads}}},
  language = {en},
  booktitle = {Proc. of the {{USENIX Security Symp}}. ({{USENIX Security}})},
  publisher = {{USENIX}},
  author = {Bashir, Muhammad Ahmad and Arshad, Sajjad and Robertson, William and Wilson, Christo},
  year = {2016},
  pages = {481-496},
  file = {/Users/chapuisbertil/Zotero/storage/BN58A3KQ/Bashir et al. - 2016 - Tracing Information Flows Between Ad Exchanges Usi.pdf}
}

@inproceedings{bashir_how_2018,
  title = {How {{Tracking Companies Circumvent Ad Blockers Using WebSockets}}},
  abstract = {In this study of 100,000 websites, we document how Advertising and Analytics (A\&A) companies have used WebSockets to bypass ad blocking, exfiltrate user tracking data, and deliver advertisements. Specifically, we leverage a longstanding bug in Chrome (the world's most popular browser) in the chrome.webRequest API that prevented blocking extensions from being able to interpose on WebSocket connections. We conducted large-scale crawls of top publishers before and after this bug was patched in April 2017 to examine which A\&A companies were using WebSockets, what information was being transferred, and whether companies altered their behavior after the patch. We find that a small but persistent group of A\&A companies use WebSockets, and that several of them are engaging in troubling behavior, such as browser fingerprinting, exfiltrating the DOM, and serving advertisements, that would have circumvented blocking due to the Chrome bug.},
  language = {en},
  booktitle = {Proc. of the {{ACM Conf}}. on {{Internet Measurement}} ({{IMC}})},
  publisher = {{ACM}},
  author = {Bashir, Muhammad Ahmad and Arshad, Sajjad and Kirda, Engin and Robertson, William and Wilson, Christo},
  year = {2018},
  pages = {7},
  file = {/Users/chapuisbertil/Zotero/storage/2ISZFEB3/Bashir et al. - How Tracking Companies Circumvent Ad Blockers Usin.pdf}
}

@inproceedings{kharaz_unveil_2016,
  title = {{{UNVEIL}}: {{A Large}}-{{Scale}}, {{Automated Approach}} to {{Detecting Ransomware}}},
  shorttitle = {{{UNVEIL}}},
  language = {en},
  booktitle = {Proc. of the {{USENIX Security Symp}}. ({{USENIX Security}})},
  publisher = {{USENIX}},
  author = {Kharaz, Amin and Arshad, Sajjad and Mulliner, Collin and Robertson, William and Kirda, Engin},
  year = {2016},
  pages = {757-772},
  file = {/Users/chapuisbertil/Zotero/storage/SG2EMMC9/Kharaz et al. - 2016 - UNVEIL A Large-Scale, Automated Approach to Det.pdf}
}

@inproceedings{arshad_large-scale_2018,
  address = {{Republic and Canton of Geneva, Switzerland}},
  series = {{{WWW}} '18},
  title = {Large-{{Scale Analysis}} of {{Style Injection}} by {{Relative Path Overwrite}}},
  isbn = {978-1-4503-5639-8},
  abstract = {Relative Path Overwrite (RPO) is a recent technique to inject style directives into sites even when no style sink or markup injection vulnerability is present. It exploits differences in how browsers and web servers interpret relative paths (i.e., path confusion) to make a HTML page reference itself as a stylesheet; a simple text injection vulnerability along with browsers\guillemotright{} leniency in parsing CSS resources results in an attacker\guillemotright{}s ability to inject style directives that will be interpreted by the browser. Even though style injection may appear less serious a threat than script injection, it has been shown that it enables a range of attacks, including secret exfiltration. In this paper, we present the first large-scale study of the Web to measure the prevalence and significance of style injection using RPO. Our work shows that around 9\% of the sites in the Alexa Top 10,000 contain at least one vulnerable page, out of which more than one third can be exploited. We analyze in detail various impediments to successful exploitation, and make recommendations for remediation. In contrast to script injection, relatively simple countermeasures exist to mitigate style injection. However, there appears to be little awareness of this attack vector as evidenced by a range of popular Content Management Systems (CMSes) that we found to be exploitable.

This article is summarized in:
the morning paper

an interesting/influential/important paper from the world of CS every weekday morning, as selected by Adrian Colyer},
  booktitle = {Proc. of the {{Int}}'l {{Conf}} on {{World Wide Web}} ({{WWW}})},
  publisher = {{ACM}},
  doi = {10.1145/3178876.3186090},
  author = {Arshad, Sajjad and Mirheidari, Seyed Ali and Lauinger, Tobias and Crispo, Bruno and Kirda, Engin and Robertson, William},
  year = {2018},
  keywords = {relative path overwrite,scriptless attack,style injection},
  pages = {237--246},
  file = {/Users/chapuisbertil/Zotero/storage/4DTYF9K5/Arshad et al. - 2018 - Large-Scale Analysis of Style Injection by Relativ.pdf}
}

@inproceedings{arshad_identifying_2016,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Identifying {{Extension}}-{{Based Ad Injection}} via {{Fine}}-{{Grained Web Content Provenance}}},
  isbn = {978-3-319-45719-2},
  abstract = {Extensions provide useful additional functionality for web browsers, but are also an increasingly popular vector for attacks. Due to the high degree of privilege extensions can hold, extensions have been abused to inject advertisements into web pages that divert revenue from content publishers and potentially expose users to malware. Users are often unaware of such practices, believing the modifications to the page originate from publishers. Additionally, automated identification of unwanted third-party modifications is fundamentally difficult, as users are the ultimate arbiters of whether content is undesired in the absence of outright malice.To resolve this dilemma, we present a fine-grained approach to tracking the provenance of web content at the level of individual DOM elements. In conjunction with visual indicators, provenance information can be used to reliably determine the source of content modifications, distinguishing publisher content from content that originates from third parties such as extensions. We describe a prototype implementation of the approach called OriginTracer for Chromium, and evaluate its effectiveness, usability, and performance overhead through a user study and automated experiments. The results demonstrate a statistically significant improvement in the ability of users to identify unwanted third-party content such as injected ads with modest performance overhead.},
  language = {en},
  booktitle = {Int'l {{Symp}}. on {{Research}} in {{Attacks}}, {{Intrusions}}, and {{Defenses}} ({{RAID}})},
  publisher = {{Springer}},
  author = {Arshad, Sajjad and Kharraz, Amin and Robertson, William},
  editor = {Monrose, Fabian and Dacier, Marc and Blanc, Gregory and {Garcia-Alfaro}, Joaquin},
  year = {2016},
  keywords = {Web security,Ad injection,Browser extension},
  pages = {415-436},
  file = {/Users/chapuisbertil/Zotero/storage/HMSCFDCX/Arshad et al. - 2016 - Identifying Extension-Based Ad Injection via Fine-.pdf}
}

@inproceedings{arshad_include_2017,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Include {{Me Out}}: {{In}}-{{Browser Detection}} of {{Malicious Third}}-{{Party Content Inclusions}}},
  isbn = {978-3-662-54970-4},
  shorttitle = {Include {{Me Out}}},
  abstract = {Modern websites include various types of third-party content such as JavaScript, images, stylesheets, and Flash objects in order to create interactive user interfaces. In addition to explicit inclusion of third-party content by website publishers, ISPs and browser extensions are hijacking web browsing sessions with increasing frequency to inject third-party content (e.g., ads). However, third-party content can also introduce security risks to users of these websites, unbeknownst to both website operators and users. Because of the often highly dynamic nature of these inclusions as well as the use of advanced cloaking techniques in contemporary malware, it is exceedingly difficult to preemptively recognize and block inclusions of malicious third-party content before it has the chance to attack the user's system.In this paper, we propose a novel approach to achieving the goal of preemptive blocking of malicious third-party content inclusion through an analysis of inclusion sequences on the Web. We implemented our approach, called Excision, as a set of modifications to the Chromium browser that protects users from malicious inclusions while web pages load. Our analysis suggests that by adopting our in-browser approach, users can avoid a significant portion of malicious third-party content on the Web. Our evaluation shows that Excision effectively identifies malicious content while introducing a low false positive rate. Our experiments also demonstrate that our approach does not negatively impact a user's browsing experience when browsing popular websites drawn from the Alexa Top 500.},
  language = {en},
  booktitle = {Int'l {{Conf}}. on {{Financial Cryptography}} and {{Data Security}} ({{FC}})},
  publisher = {{Springer}},
  author = {Arshad, Sajjad and Kharraz, Amin and Robertson, William},
  editor = {Grossklags, Jens and Preneel, Bart},
  year = {2017},
  keywords = {Web security,Machine learning,Malvertising},
  pages = {441-459},
  file = {/Users/chapuisbertil/Zotero/storage/3ESLHAF8/Arshad et al. - 2017 - Include Me Out In-Browser Detection of Malicious .pdf}
}

@article{levy_stickler_2016,
  title = {Stickler: {{Defending}} against {{Malicious Content Distribution Networks}} in an {{Unmodified Browser}}},
  volume = {14},
  issn = {1540-7993},
  shorttitle = {Stickler},
  abstract = {Website publishers can derive enormous performance benefits and cost savings by directing traffic to their sites through content distribution networks (CDNs). However, publishers who use CDNs must trust they won't modify the site's JavaScript, CSS, images, or other media en route to end users. A CDN that violates this trust could inject ads into websites, downsample media to save bandwidth, or, worse, inject malicious JavaScript code to steal user secrets it couldn't otherwise access. The authors present Stickler, a system for website publishers that guarantees the end-to-end authenticity of content served to users that simultaneously lets publishers reap the benefits of CDNs. Crucially, Stickler achieves these guarantees without requiring modifications to the browser.},
  number = {2},
  journal = {IEEE Security Privacy},
  doi = {10.1109/MSP.2016.32},
  author = {Levy, A. and {Corrigan-Gibbs}, H. and Boneh, D.},
  month = mar,
  year = {2016},
  keywords = {security of data,Servers,security,Privacy,Computer security,Malware,Browsers,Cryptography,browser,CDN,computer networks,content distribution network,content distribution networks,Content distribution networks,end-to-end content authenticity,malicious CDN,online front-ends,Stickler,Web,Web cryptography,Website publishers},
  pages = {22-28},
  file = {/Users/chapuisbertil/Zotero/storage/X5DT6LX6/Levy et al. - 2016 - Stickler Defending against Malicious Content Dist.pdf;/Users/chapuisbertil/Zotero/storage/HHMW98F7/7448352.html}
}

@incollection{bodden_control_2017,
  address = {{Cham}},
  title = {Control {{What You Include}}!},
  volume = {10379},
  isbn = {978-3-319-62104-3 978-3-319-62105-0},
  abstract = {Third party tracking is the practice by which third parties recognize users accross different websites as they browse the web. Recent studies show that more than 90\% of Alexa top 500 websites [38] contain third party content that is tracking its users across the web. Website developers often need to include third party content in order to provide basic functionality. However, when a developer includes a third party content, she cannot know whether the third party contains tracking mechanisms. If a website developer wants to protect her users from being tracked, the only solution is to exclude any third-party content, thus trading functionality for privacy. We describe and implement a privacypreserving web architecture that gives website developers a control over third party tracking: developers are able to include functionally useful third party content, the same time ensuring that the end users are not tracked by the third parties.},
  language = {en},
  booktitle = {Engineering {{Secure Software}} and {{Systems}}},
  publisher = {{Springer}},
  author = {Som{\'e}, Doli{\`e}re Francis and Bielova, Nataliia and Rezk, Tamara},
  editor = {Bodden, Eric and Payer, Mathias and Athanasopoulos, Elias},
  year = {2017},
  pages = {115-132},
  file = {/Users/chapuisbertil/Zotero/storage/QY2Q3TR7/Somé et al. - 2017 - Control What You Include!.pdf},
  doi = {10.1007/978-3-319-62105-0_8}
}

@inproceedings{some_empoweb_2019,
  address = {{San Francisco, CA, USA}},
  title = {{{EmPoWeb}}: {{Empowering Web Applications}} with {{Browser Extensions}}},
  booktitle = {Proc. of the {{IEEE Symp}}. on {{Security}} and {{Privacy}} ({{S}}\&{{P}})},
  publisher = {{IEEE}},
  doi = {10.1109/SP.2019.00058},
  author = {Som{\'e}, D.},
  month = may,
  year = {2019},
  keywords = {-attacks-and-defenses,-malware-and-unwanted-software,-mobile-and-web-security-and-privacy,-privacy-technologies-and-mechanisms,application-security},
  pages = {1073-1091},
  file = {/Users/chapuisbertil/Zotero/storage/5NADTGRJ/Somé - 2019 - EmPoWeb Empowering Web Applications with Browser .pdf;/Users/chapuisbertil/Zotero/storage/JG9J9Q27/Somé - EmPoWeb Empowering Web Applications with Browser .pdf}
}

@inproceedings{calzavara_content_2016,
  address = {{New York, NY, USA}},
  series = {{{CCS}} '16},
  title = {Content {{Security Problems}}?: {{Evaluating}} the {{Effectiveness}} of {{Content Security Policy}} in the {{Wild}}},
  isbn = {978-1-4503-4139-4},
  shorttitle = {Content {{Security Problems}}?},
  abstract = {Content Security Policy (CSP) is an emerging W3C standard introduced to mitigate the impact of content injection vulnerabilities on websites. We perform a systematic, large-scale analysis of four key aspects that impact on the effectiveness of CSP: browser support, website adoption, correct configuration and constant maintenance. While browser support is largely satisfactory, with the exception of few notable issues, our analysis unveils several shortcomings relative to the other three aspects. CSP appears to have a rather limited deployment as yet and, more crucially, existing policies exhibit a number of weaknesses and misconfiguration errors. Moreover, content security policies are not regularly updated to ban insecure practices and remove unintended security violations. We argue that many of these problems can be fixed by better exploiting the monitoring facilities of CSP, while other issues deserve additional research, being more rooted into the CSP design.},
  booktitle = {Proc. of the {{ACM Conf}}. on {{Computer}} and {{Communications Security}} ({{CCS}})},
  publisher = {{ACM}},
  doi = {10.1145/2976749.2978338},
  author = {Calzavara, Stefano and Rabitti, Alvise and Bugliesi, Michele},
  year = {2016},
  keywords = {measurement,content security policy},
  pages = {1365--1375},
  file = {/Users/chapuisbertil/Zotero/storage/2L6GRHBP/Calzavara et al. - 2016 - Content Security Problems Evaluating the Effecti.pdf}
}

@inproceedings{some_content_2017,
  address = {{Republic and Canton of Geneva, Switzerland}},
  series = {{{WWW}} '17},
  title = {On the {{Content Security Policy Violations Due}} to the {{Same}}-{{Origin Policy}}},
  isbn = {978-1-4503-4913-0},
  abstract = {Modern browsers implement different security policies such as the Content Security Policy (CSP), a mechanism designed to mitigate popular web vulnerabilities, and the Same Origin Policy (SOP), a mechanism that governs interactions between resources of web pages. In this work, we describe how CSP may be violated due to the SOP when a page contains an embedded iframe from the same origin. We analyse 1 million pages from 10,000 top Alexa sites and report that at least 31.1\% of current CSP-enabled pages are potentially vulnerable to CSP violations. Further considering real-world situations where those pages are involved in same-origin nested browsing contexts, we found that in at least 23.5\% of the cases, CSP violations are possible. During our study, we also identified a divergence among browsers implementations in the enforcement of CSP in srcdoc sandboxed iframes, which actually reveals a problem in Gecko-based browsers CSP implementation. To ameliorate the problematic conflicts of the security mechanisms, we discuss measures to avoid CSP violations.},
  booktitle = {Proc. of the {{Int}}'l {{Conf}} on {{World Wide Web}} ({{WWW}})},
  publisher = {{ACM}},
  doi = {10.1145/3038912.3052634},
  author = {Some, Doli{\`e}re Francis and Bielova, Nataliia and Rezk, Tamara},
  year = {2017},
  keywords = {content security policy,same origin policy,security and privacy,web application security},
  pages = {877--886},
  file = {/Users/chapuisbertil/Zotero/storage/44IVFEJJ/Some et al. - 2017 - On the Content Security Policy Violations Due to t.pdf}
}

@inproceedings{krombholz_if_2019,
  title = {``{{If HTTPS Were Secure}}, {{I Wouldn}}'t {{Need 2FA}}''- {{End User}} and {{Administrator Mental Models}} of {{HTTPS}}},
  abstract = {HTTPS is one of the most important protocols used to secure communication and is, fortunately, becoming more pervasive. However, especially the long tail of websites is still not sufficiently secured. HTTPS involves different types of users, e.g., end users who are forced to make security decisions when faced with warnings or administrators who are required to deal with cryptographic fundamentals and complex decisions concerning compatibility.},
  language = {en},
  booktitle = {Proc. of the {{IEEE Symp}}. on {{Security}} and {{Privacy}} ({{SP}})},
  publisher = {{IEEE}},
  author = {Krombholz, Katharina and Busse, Karoline and Pfeffer, Katharina and Smith, Matthew},
  year = {2019},
  pages = {18},
  file = {/Users/chapuisbertil/Zotero/storage/TTJRUSSI/Krombholz et al. - “If HTTPS Were Secure, I Wouldn’t Need 2FA”- End U.pdf}
}

@article{romagna_hacktivism_2017,
  title = {Hacktivism and Website Defacement: Motivations, Capabilities and Potential Threats},
  volume = {1},
  abstract = {Hacktivism and website defacement seem often to be linked: websites are defaced by hacktivists on a daily basis for many different reasons. However, due to a lack of studies of this phenomenon, it remains unclear as to what, exactly, their socio-psychological motivations are, what their modus operandi is, and whether the combination of these factors poses a serious threat to corporations and governmental organizations. In order to answer these questions, this paper provides a qualitative analysis of the motives and intentions of hacktivists, and a qualitative analysis of their modus operandi. It seems that hacktivists who deface websites have multiple ideological and psychological motivations for their actions. Although the socio-political motivations appear to be the most important, other triggers \textendash{} such as thrill seeking and increasing self-esteem \textendash{} also play a relevant part. The investigation into the modus operandi has revealed that hacktivists often use known and relatively unsophisticated vulnerabilities and techniques. In addition, they use publicly available tools, but are also able to create their own. Targets seem to be chosen based either on how easy they are to hack and/or on the potential amount of attention the defacement is likely to receive. The methodology of this research involves an extensive review of the existing literature on the topic, corroborated by several interviews with hacktivists and experts in the field of information and cybersecurity. The researchers conducted an analysis of forensic data gained from a honeypot server created ad hoc for this research, and examined technical data from over 7 million defacements based on the dataset of the Zone-H Defacement Archive.},
  language = {en},
  journal = {27th Virus Bulletin International Conference},
  author = {Romagna, Marco and {van den Hout}, Niek Jan},
  year = {2017},
  pages = {11},
  file = {/Users/chapuisbertil/Zotero/storage/G2MFXU99/Romagna - HACKTIVISM AND WEBSITE DEFACEMENT MOTIVATIONS, CA.pdf}
}

@article{felt_measuring_nodate,
  title = {Measuring {{HTTPS Adoption}} on the {{Web}}},
  abstract = {HTTPS ensures that the Web has a base level of privacy and integrity. Security engineers, researchers, and browser vendors have long worked to spread HTTPS to as much of the Web as possible via outreach efforts, developer tools, and browser changes. How much progress have we made toward this goal of widespread HTTPS adoption? We gather metrics to benchmark the status and progress of HTTPS adoption on the Web in 2017. To evaluate HTTPS adoption from a user perspective, we collect large-scale, aggregate user metrics from two major browsers (Google Chrome and Mozilla Firefox). To measure HTTPS adoption from a Web developer perspective, we survey server support for HTTPS among top and long-tail websites. We draw on these metrics to gain insight into the current state of the HTTPS ecosystem.},
  language = {en},
  author = {Felt, Adrienne Porter and Barnes, Richard and King, April and Palmer, Chris and Bentzel, Chris and Tabriz, Parisa},
  pages = {16},
  file = {/Users/chapuisbertil/Zotero/storage/BBBJHX2F/Felt et al. - Measuring HTTPS Adoption on the Web.pdf}
}


